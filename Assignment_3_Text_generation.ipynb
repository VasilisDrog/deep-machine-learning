{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "HMT0d4aAbN1_",
        "GR9l7fx3O0-j",
        "8kU-JTpveBgn",
        "d3-3Yj9hhEhB",
        "pZ39MkmEwFa7"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VasilisDrog/deep-machine-learning/blob/master/Assignment_3_Text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 3: Algorithms for text generation\n",
        "\n",
        "In this assignment, we will explore using trained language models to generate text. In particular, we will work with a recent model called [Generative Pre-trained Transformer, version 2 \\(GPT-2\\)](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) that was published in 2019 by OpenAI.\n",
        "\n",
        "As language models are probabilistic models of text, there are different methods of generating (also known as _decoding_) text strings from the model, as we have seen in [one of the lectures](http://www.cse.chalmers.se/~richajo/dat450/lectures/l7/m7_3.pdf). You will implement some of the most common decoding methods in this assignment, and later reflect on the qualitative aspects of the different methods.\n",
        "\n",
        "**Note:** It will be important to use a GPU with a large memory, such as provided on Colab. Please enable the GPU runtime by going to _Runtime -> Change Runtime type -> GPU_.\n",
        "\n",
        "**Note:** Implementations of the generation algorithms you code here already exist in the Huggingface library. In a real use case, you would typically just call `generate`. These reimplementations are for pedagogical purposes.  "
      ],
      "metadata": {
        "id": "z2L91fr4uGio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's start by importing the PyTorch library:\n",
        "\n",
        "import torch\n",
        "torch.set_grad_enabled(False) # since we will not be updating any models..."
      ],
      "metadata": {
        "id": "RJT7isgJRZ_X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4a4f432-39e0-481d-a42c-c2ef78f49090"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x7f236348ae10>"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The GPT-2 Language Model\n",
        "\n",
        "In the GPT-2, a _Transformer Decoder_ is used to model the conditional probability $P(x_i | x_1, ..., x_{i-1})$ using large quantities of text data. As training big language models are typically very computationally expensive, we will not train our own in this assignment, but use a pre-trained one instead. For this we will need to install a separate package, called `transformers`."
      ],
      "metadata": {
        "id": "pInbcRpvODQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "m0igih6ZS9Al",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d131dfe6-e505-489c-d72a-644cfd01714a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The GPT-2 model comes with its own tokenizer, which we will need to load:"
      ],
      "metadata": {
        "id": "JWonc2QlTaWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer"
      ],
      "metadata": {
        "id": "Z274C44T5QDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgYlfHZRAprm"
      },
      "outputs": [],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer.encode(\"NLP stands for natural\", return_tensors=\"pt\").to(\"cuda\")\n",
        "input_ids"
      ],
      "metadata": {
        "id": "o6nPe90iUaW6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b089f3f8-782a-4d96-eff5-a8fe1726c705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   45, 19930,  6296,   329,  3288]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like the tokenizers we've seen earlier in the course, it maps a text string to a sequence of tokens (integers) from a fixed size vocabulary. Note that `input_ids` is two-dimensional, where the first dimension is the batch dimension, and second dimension is the sequence dimension.\n",
        "The tokenizer can also decode the integers back to the string representation:"
      ],
      "metadata": {
        "id": "Qau4qvB2Wiyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(input_ids[0])"
      ],
      "metadata": {
        "id": "1BHVbWLa9ssv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "87f54f4b-2c2b-4d7a-e43b-a3b2ecf9bb19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'NLP stands for natural'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now download the trained model. As we will work with a large model in this assignment (several hundreds of millions of parameters), using a GPU will _greatly_ speed up predictions. "
      ],
      "metadata": {
        "id": "P4Msvqob9j5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\").to(\"cuda\").eval()"
      ],
      "metadata": {
        "id": "Cuyqt4X9XBKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For your curiosity, you can optionally print the number of parameters in each layer and the total number of parameters:"
      ],
      "metadata": {
        "id": "YJytCxvYTLWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# total_parameters = 0\n",
        "# for name, par in model.named_parameters():\n",
        "#  n_par = 1\n",
        "#  for d in par.shape:\n",
        "#    n_par *= d\n",
        "#  print(f'{name}: {n_par} parameters')\n",
        "#  total_parameters += n_par\n",
        "# print(f'Total number of parameters: {total_parameters}')"
      ],
      "metadata": {
        "id": "yeXUeGlnXHuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the model loaded, let's use it for predicting the next token of our tokenized `input_ids` from above:"
      ],
      "metadata": {
        "id": "O3IcB6Rf1-7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model(input_ids=input_ids)\n",
        "\n",
        "predictions.logits"
      ],
      "metadata": {
        "id": "LRsY9J1214od",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dabe855-846f-422a-9a44-e64c71b2bf46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 1.7978,  2.8982,  1.6116,  ..., -5.3930, -5.0412,  0.7757],\n",
              "         [ 2.0065,  5.6440,  0.4546,  ..., -3.7273, -6.4793,  1.1439],\n",
              "         [ 2.4830,  2.4556, -1.5678,  ..., -6.9141, -6.2589,  1.0836],\n",
              "         [ 1.6584,  4.0486, -3.9575,  ..., -6.5006, -7.1734,  1.3958],\n",
              "         [ 2.0553,  3.9053, -1.5021,  ..., -2.4546, -7.9215,  1.0312]]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.logits.shape"
      ],
      "metadata": {
        "id": "n6d0sDZZ2iQ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9577edb-f7d2-4224-fb3f-2429657de222"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What we get from the model are the unnormalized log probabilities, called _logits_. \n",
        "\n",
        "**Self-check:** Look at the shape of `predictions.logits` from above, what do the three dimensions represent? 1 is batch size, 5 the lengt of the prompt, 50257 is the size of the vocabulary\n",
        "\n",
        "**Your work:** How can we, from `predictions.logits`, compute the actual probability distribution of the next word in the sequence `NLP stands for natural ____`? The distribution should be over the entire vocabulary, and be valid probabilities that sum to one."
      ],
      "metadata": {
        "id": "xhXpfjNG2_R-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input1 = predictions.logits[0, -1, :] # \n",
        "next_token_prob= torch.nn.functional.softmax(input1, dim=-1)\n",
        "\n",
        "next_token_prob.shape"
      ],
      "metadata": {
        "id": "-zYkIDnA2-pB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "995ff089-3c5a-4106-fd31-012e1e1921aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50257])"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# These tests should pass without modifications\n",
        "assert next_token_prob.shape == torch.Size([tokenizer.vocab_size])\n",
        "assert abs(next_token_prob.sum() - 1.0) < 0.01\n",
        "assert all(next_token_prob >= 0)"
      ],
      "metadata": {
        "id": "OQpZiarTZLb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your work:** Compute the top 5 most probable next tokens, based on the `next_token_prob` distribution. \n",
        "\n",
        "**Hint**: the function [`topk`](https://pytorch.org/docs/stable/generated/torch.topk.html) will be useful here."
      ],
      "metadata": {
        "id": "EhlGz05G7phK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_5_next_tokens =  torch.topk(input1, 5).indices # show where the token are in the vocab\n",
        "print(top_5_next_tokens)"
      ],
      "metadata": {
        "id": "jWTNJo2Yatts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c56c38aa-b303-48d1-b3c7-dc14e4e34356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 3303, 15417,  1692,  8950,    12], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can again use `tokenizer.decode` to map the integer-encoded tokens back to strings."
      ],
      "metadata": {
        "id": "CfoIKaHkT5-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index in top_5_next_tokens:\n",
        "  print(f\"{tokenizer.decode([index])}\")\n",
        "print()"
      ],
      "metadata": {
        "id": "E9XYdwTkAR3r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18809c17-a963-4c67-88dd-c842ae2439ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " language\n",
            " Language\n",
            " human\n",
            " languages\n",
            "-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could now decide to, for example, pick the token id with the highest probability, append that to our input, and run through the model again to compute the distribution for the next token again.\n",
        "\n",
        "**Your work:** Take the higest predicted token from `top_5_next_tokens` and append to `input_ids`\n",
        "\n",
        "**Hint:** The function [`torch.cat`](https://pytorch.org/docs/stable/generated/torch.cat.html) could be useful here."
      ],
      "metadata": {
        "id": "kwpfDMWv_KpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(input[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8lLP_LAIYBw",
        "outputId": "787f88d0-6462-4fbc-8244-63563455f6a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.0553, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_input_ids = torch.cat((input_ids, top_5_next_tokens[0][None, None]), dim=-1 )"
      ],
      "metadata": {
        "id": "Uv2j0_fTC0bB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see that the prediction is sensible, you can convert the integer-encoded tensor back into text:"
      ],
      "metadata": {
        "id": "E0KWVpQWNMfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(new_input_ids[0])"
      ],
      "metadata": {
        "id": "FLU2ykBYHTlB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "355530fd-da1b-4951-cb14-a5f201dfa585"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'NLP stands for natural language'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# These tests should pass without modifications\n",
        "assert new_input_ids.shape == torch.Size([1, input_ids.shape[1] + 1])\n",
        "assert new_input_ids[0, -1] == top_5_next_tokens[0]"
      ],
      "metadata": {
        "id": "GYQc7jBIEmTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your work:** Like above, compute a new distribution for the next token and print the top 5 most probable next tokens"
      ],
      "metadata": {
        "id": "NzhmkbRoFuQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_decode = tokenizer.decode(new_input_ids[0])\n",
        "print(new_decode)\n",
        "\n",
        "input_ids2 = tokenizer.encode(new_decode, return_tensors=\"pt\").to(\"cuda\")\n",
        "predictions2 = model(input_ids=input_ids2)\n",
        "print(input_ids2)\n",
        "\n",
        "input2 = predictions2.logits[0, -1, :]\n",
        "next_token_prob= torch.nn.functional.softmax(input2, dim=-1)\n",
        "top_5_next_tokens2 =  torch.topk(input2,5).indices\n",
        "\n",
        "\n",
        "for index in top_5_next_tokens2:\n",
        "  print(f\"{tokenizer.decode([index])}\")\n",
        "\n",
        "\n",
        "new_input_ids2 = torch.cat((input_ids2, top_5_next_tokens2[0][None, None]), dim=-1 )\n",
        "\n",
        "tokenizer.decode(new_input_ids2[0])"
      ],
      "metadata": {
        "id": "mo6-TiumGR3_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "0bffd2ce-30bf-4634-e853-fcf1be6bb2d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLP stands for natural language\n",
            "tensor([[   45, 19930,  6296,   329,  3288,  3303]], device='cuda:0')\n",
            " processing\n",
            " understanding\n",
            " perception\n",
            " parsing\n",
            " generation\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'NLP stands for natural language processing'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating from a language model\n",
        "\n",
        "What we just did can be formalized into a general algorithm to generate text from a language model:\n",
        "\n",
        "1. Start with some text to be _continued_. We will denote this initial text as a _prompt_: $x_1, ..., x_i$\n",
        "2. Use the language model to compute the next token probabilities: $P(x_{i+1} | x_1, ..., x_i)$\n",
        "3. Based on the distribution, pick some next token $x_{i+1}$ and append to the input\n",
        "4. Repeat from step 2 until a stopping criterion is met."
      ],
      "metadata": {
        "id": "-2s2Noe2-8JO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An important decision when generating from language models is what strategy you apply for picking next tokens (step 3). We will implement and experiment with different such strategies and you will in the individual reflection discuss pros and cons of each, and how these differ from each other.\n",
        "\n",
        "We begin by defining an abstract decoding strategy class, that has a method `step` which takes the `logits` and `input_ids` at some step. `step(...)` returns updated `input_ids` to be used in the next step."
      ],
      "metadata": {
        "id": "YOxNKTKI-eLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class DecodingStrategy(ABC):\n",
        "\n",
        "  @abstractmethod\n",
        "  def step(self, logits, input_ids):\n",
        "    \"\"\"\n",
        "    This method takes next token logits and input_ids and applies some strategy to update the input_ids.\n",
        "    It returns the updated input ids.\n",
        "\n",
        "    Args:\n",
        "      logits:    3d float tensor\n",
        "      input_ids: 2d int tensor\n",
        "\n",
        "    Returns:\n",
        "      next_input_ids: 2d int tensor\n",
        "    \"\"\"\n",
        "    raise NotImplementedError()"
      ],
      "metadata": {
        "id": "TNa0vxutLxqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will implement a stopping criterion. In this assignment we will stop when the model has generated X number of sentences. We define sentence boundaries by the period token:"
      ],
      "metadata": {
        "id": "LuhKGbmRu3Uv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(\".\")"
      ],
      "metadata": {
        "id": "hUoLnLUbvz4q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c449cf5-45eb-478f-e293-7b38fcb6510c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[13]"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your work:** Implement the following function, that returns the number of completed sentences in each batch sequence in `input_ids`:"
      ],
      "metadata": {
        "id": "UOOTkIx4v6QZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_num_sentences(input_ids):\n",
        "  \"\"\"\n",
        "  Returns an integer tensor of shape input_ids.shape[0], that tells how many completed \n",
        "  sentences there are in each batch sequence\n",
        "  \"\"\"\n",
        "\n",
        "  # WRITE CODE HERE\n",
        "  new_input_shape = input_ids.size()\n",
        "  print(input_ids)\n",
        "  # return new_input_shape\n",
        "\n",
        "  # raise NotImplementedError()\n"
      ],
      "metadata": {
        "id": "riYe7Uu2wKrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This test should pass without modification\n",
        "test_input_ids = tokenizer([\"This sequence has zero completed sentences\", \"Here is one completed sentence. Here is another.\"], return_tensors=\"pt\", padding=True).input_ids\n",
        "get_num_sentences(test_input_ids)\n",
        "\n",
        "# assert torch.equal(get_num_sentences(test_input_ids), torch.tensor([0, 2]))\n",
        "# print(test_input_ids)\n",
        "# torch.tensor([0, 2])"
      ],
      "metadata": {
        "id": "fZRo-yc0wmre",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c4671c4-836b-4f77-ea4a-f0498258e52a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1212,  8379,   468,  6632,  5668, 13439, 50256, 50256, 50256, 50256],\n",
            "        [ 4342,   318,   530,  5668,  6827,    13,  3423,   318,  1194,    13]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your work:** Implement the stopping criterion function below, that returns a boolean vector indicating if each batch sequence has at least `n` or more sentences. Use the `get_num_sentences` function from above."
      ],
      "metadata": {
        "id": "WtokkoSzzYp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def has_n_sentences(input_ids, n):\n",
        "  # WRITE CODE HERE\n",
        "  \n",
        "  raise NotImplementedError()"
      ],
      "metadata": {
        "id": "uTvdVZ9SzyL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This test should pass without modification\n",
        "assert torch.equal(\n",
        "    has_n_sentences(test_input_ids, n=2), \n",
        "    torch.tensor([False, True])\n",
        ")"
      ],
      "metadata": {
        "id": "1eMTsEqZ0Njt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "05de54a6-9992-4f61-d04f-9d738db07e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-139-86ef60043c00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This test should pass without modification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m assert torch.equal(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mhas_n_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;32m<ipython-input-138-4515a5d92725>\u001b[0m in \u001b[0;36mhas_n_sentences\u001b[0;34m(input_ids, n)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;31m# WRITE CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a prompt and some strategy, we can implement the generation algorithm. The generation stops when all sequences in the batch are done (according to the stopping criterion), or when a maximum generation length is reached. "
      ],
      "metadata": {
        "id": "edARYwQBUq4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "def generate(prompt, strategy, stopping_criterion, max_length=100, print_output=True):\n",
        "  \"\"\"\n",
        "  TODO write docstring. Remove types.\n",
        "  \"\"\"\n",
        "\n",
        "  # Step 1: \n",
        "  encoded_prompt = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "  input_ids = encoded_prompt\n",
        "\n",
        "  while not torch.all(stopping_criterion(input_ids)) and input_ids.shape[1] < encoded_prompt.shape[1] + max_length:\n",
        "    # Step 2: Get next token logits\n",
        "    predictions = model(input_ids=input_ids)\n",
        "\n",
        "    # Step 3: Apply decoding strategy to update input_ids\n",
        "    input_ids = strategy.step(predictions.logits, input_ids)\n",
        "\n",
        "    # Print generated string(s) so far\n",
        "    if print_output:\n",
        "      clear_output()\n",
        "      for batch_idx in range(input_ids.shape[0]):\n",
        "        print(tokenizer.decode(input_ids[batch_idx], skip_special_tokens=True))\n",
        "        print(\"----------------------------------------------------------------\")\n",
        "\n",
        "  return input_ids"
      ],
      "metadata": {
        "id": "Dik42WLgMwzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test our generation algorithm, we can implement a dummy strategy, that disregards the logits, and just picks a random token from the vocabulary as next token."
      ],
      "metadata": {
        "id": "3kNcHgPMWtk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyStrategy(DecodingStrategy):\n",
        "  def step(self, logits, input_ids):\n",
        "    next_tokens = torch.randint(low=0, high=tokenizer.vocab_size, size=[input_ids.shape[0]]).to(input_ids.device)\n",
        "    new_input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
        "    return new_input_ids\n"
      ],
      "metadata": {
        "id": "foslmy0RVTqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "dummy_strategy = DummyStrategy()\n",
        "stopping_criterion = partial(has_n_sentences, n=2)  # Returns a new function where argument n is set to 2\n",
        "\n",
        "_ = generate(\n",
        "    prompt=\"NLP stands for natural\", \n",
        "    strategy=dummy_strategy, \n",
        "    stopping_criterion=stopping_criterion,\n",
        "    max_length=20\n",
        ")"
      ],
      "metadata": {
        "id": "Po-lU2i6AAR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, the generated string are just rubbish."
      ],
      "metadata": {
        "id": "US_KmLYnXpDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating from a language model can be seen as a search problem, where  all possible token strings span a large search tree. \n",
        "\n",
        "![picture](https://huggingface.co/blog/assets/02_how-to-generate/greedy_search.png)\n",
        "\n",
        "A good idea is to run a search to find the string that is the _most probable_ under the language model, i.e:\n",
        "\n",
        "$\\DeclareMathOperator*{\\argmax}{argmax}$\n",
        "\n",
        "\\begin{align}\n",
        "  x_{i+1}^*, ..., x_n^* &=  \\argmax_{x_{i+1}, ..., x_n}  P(x_{i+1}, ..., x_n | x_1, ..., x_i) \\\\\n",
        "  &= \\argmax_{x_{i+1}, ..., x_n} \\prod_{i'=i}^n P(x_{i'+1} | x_1, ..., x_{i'})\n",
        "\\end{align}\n",
        "\n",
        "However, assuming we cannot run a brute force search, what search algorithms are there that we can apply?\n"
      ],
      "metadata": {
        "id": "mtTNsK0AeJiD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Greedy decoding\n",
        "\n",
        "What we did in the beginning, i.e. picking the most probable token, is known as the _greedy_ decoding strategy. That means we approximate the argmax by taking the most probable token at each step. The algorithm is described conceptually on slides 6-14 in [the lecture](http://www.cse.chalmers.se/~richajo/dat450/lectures/l7/m7_3.pdf), but please keep in mind that the pseudocode given in the lecture generates just a single text.\n",
        "\n",
        "**Your work:** Implement the greedy strategy in the class skeleton below:"
      ],
      "metadata": {
        "id": "HMT0d4aAbN1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GreedyStrategy(DecodingStrategy):\n",
        "\n",
        "  def step(self, logits, input_ids):\n",
        "    # WRITE CODE HERE\n",
        "    # next_input_ids = ...\n",
        "    raise NotImplementedError()\n",
        "    return next_input_ids\n"
      ],
      "metadata": {
        "id": "eHhk295lWjh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This test should pass without modification\n",
        "greedy_strategy = GreedyStrategy()\n",
        "test_input_ids = torch.tensor([[1, 2, 3]])\n",
        "test_logits = torch.tensor([[[0.1, 0.1, 0.1, 0.1, 0.6],\n",
        "                             [0.1, 0.1, 0.1, 0.6, 0.1],\n",
        "                             [0.1, 0.1, 0.6, 0.1, 0.1]]])\n",
        "test_new_input_ids = greedy_strategy.step(test_logits, test_input_ids)\n",
        "assert torch.equal(test_new_input_ids, torch.tensor([[1, 2, 3, 2]]))"
      ],
      "metadata": {
        "id": "lvAob8FjkUiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, try generating some text using this strategy:"
      ],
      "metadata": {
        "id": "ucn0YiRdlv-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generated_ids = generate(\"NLP stands for natural\", greedy_strategy, stopping_criterion=partial(has_n_sentences, n=2))"
      ],
      "metadata": {
        "id": "9HwkMY8JVrpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now implement a method to compute the log probablity of the generated string, i.e. $\\log P(x_{i+1}, ..., x_n | x_1, ..., x_i)$"
      ],
      "metadata": {
        "id": "PXk9VtcjoeS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_joint_log_probability(logits, input_ids):\n",
        "  labels = input_ids[:, 1:].clone().reshape(-1)\n",
        "  labels[labels == tokenizer.pad_token_id] = -100\n",
        "  logits = logits[:, :-1, :].reshape(-1, logits.shape[-1])\n",
        "  normalized_log_probs = torch.nn.functional.cross_entropy(logits, labels, reduction=\"none\")\n",
        "  normalized_log_probs = normalized_log_probs.reshape(input_ids.shape[0], -1)\n",
        "  return -normalized_log_probs.sum(-1)"
      ],
      "metadata": {
        "id": "BF39gOECoUqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compute the joint log probability of the generated text:"
      ],
      "metadata": {
        "id": "JEjH3glg53NS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model(input_ids=generated_ids)\n",
        "greedy_joint_logprob = get_joint_log_probability(predictions.logits, generated_ids)\n",
        "print('Joint log probability of the text using greedy search:', greedy_joint_logprob[0].item())"
      ],
      "metadata": {
        "id": "npKrKAZg5HCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The higher this value is, the more likely the generated string is, under the language model. We will compare this value to the corresponding value for our next decoding strategy, which is **beam search**."
      ],
      "metadata": {
        "id": "G1uTyB2n_jnz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beam search\n",
        "\n",
        "While greedy search finds strings that have high probability under the model, it often takes suboptimal decisions where a low probability word might yield a greater joint probability in the end.\n",
        "\n",
        "In [beam search](https://en.wikipedia.org/wiki/Beam_search), we run multiple search _alternatives_ (beams) in parallel and at each step, we select the $k$ most probable alternatives to pass on to the next step. Conceptually, this algorithm has been described in slides 18-20 of [the lecture](http://www.cse.chalmers.se/~richajo/dat450/lectures/l7/m7_3.pdf), but our code will differ a bit from the conceptual pseudocode because of PyTorch technicalities and because of the stopping criterion.\n",
        "\n",
        "**Your work:** Implement the beam search strategy in the skeleton below. You will find a comment `# WRITE CODE HERE` where you are expected to add your own code."
      ],
      "metadata": {
        "id": "uaG1x9eQm8Qy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BeamSearchStrategy(DecodingStrategy):\n",
        "  def __init__(self, num_beams: int, stopping_criterion):\n",
        "    self.num_beams = num_beams\n",
        "    self.stopping_criterion = stopping_criterion\n",
        "\n",
        "  def step(self, logits, input_ids):\n",
        "    # Let's define some auxiliary variables we will use in sanity checks.\n",
        "    n_beams, n_tokens, voc_size = logits.shape\n",
        "\n",
        "    # *YOUR WORK*: Compute log prob for the beams from the previous step\n",
        "    # The result is a tensor of shape n_beams.\n",
        "    # TODO: student code here\n",
        "    log_probs = # WRITE CODE HERE\n",
        "    assert(log_probs.shape == torch.Size([n_beams]))\n",
        "\n",
        "    # Apply the stopping criterion to see which beams are finished.\n",
        "    # The result is a boolean tensor of shape n_beams.\n",
        "    is_finished = self.stopping_criterion(input_ids)    \n",
        "    is_not_finished = ~is_finished\n",
        "\n",
        "    # Select the beams that are finished and unfinished, respectively.\n",
        "    finished_ids = input_ids[is_finished]\n",
        "    unfinished_ids = input_ids[is_not_finished]\n",
        "    n_unfinished = unfinished_ids.shape[0]\n",
        "\n",
        "    # ... and the log probabilities for the finished and unfinished beams.\n",
        "    finished_log_probs = log_probs[is_finished]\n",
        "    unfinished_log_probs = log_probs[is_not_finished]\n",
        "\n",
        "    # *YOUR WORK*: First, convert the logits for the next token prediction into log probabilities.\n",
        "    # *HINT*: You can use log_softmax for this.\n",
        "    log_probs_next_token =  # WRITE CODE HERE\n",
        "    assert(log_probs_next_token.shape == torch.Size([n_unfinished, voc_size]))\n",
        "\n",
        "    # *YOUR WORK*: Then, add the next token log probabilities to the log probabilities for the\n",
        "    # previous unfinished beams.\n",
        "    #\n",
        "    # *HINT*: This requires a PyTorch tensor trick: what we want to do is to add a beam\n",
        "    # log-probability to *each* next token log-probability for this beam.\n",
        "    # The shape of unfinished_log_probs is [n_unfinished] while the shape of \n",
        "    # log_probs_next_token is [n_unfinished, voc_size].\n",
        "    # To do this, view unfinished_log_probs as a tensor of shape [n_unfinished, 1]\n",
        "    # by writing as follows: unfinished_log_probs[:, None]\n",
        "    # When both tensors are 2-dimensional, they can be summed: in PyTorch, if we add\n",
        "    # a tensor of shape [m, n] to one of shape [m, 1], the second tensor will be\n",
        "    # treated as if it were of shape [m, n] as well (with all rows copied).\n",
        "    # \n",
        "    log_probs_beams_expanded = # WRITE CODE HERE\n",
        "    assert(log_probs_beams_expanded.shape == torch.Size([n_unfinished, voc_size]))\n",
        "\n",
        "    # *YOUR WORK*: Now, sort the log probabilities for the expanded beams in descending order.\n",
        "    # *HINT*: first flatten the tensor so that it has the shape n_unfinished*voc_size.\n",
        "    # *HINT*: PyTorch has a built-in sort function that you can read about here:\n",
        "    # See https://pytorch.org/docs/stable/generated/torch.sort.html#torch.sort.\n",
        "    expanded_sorted = # WRITE CODE HERE\n",
        "    assert(expanded_sorted.values.shape == torch.Size([n_unfinished*voc_size]))\n",
        "\n",
        "    # Here, take some time to understand what was returned by the sorting function.\n",
        "    # This function returns two tensors, one (.values) containing the sorted values and \n",
        "    # another (.indices) containing the indices of the original positions of what was sorted.\n",
        "\n",
        "    # We will now carry out the step to compute the updated beam.\n",
        "    # \n",
        "    next_unfinished_idx = 0\n",
        "    next_finished_idx = 0\n",
        "\n",
        "    # This list will keep the selected beams.\n",
        "    beams = []\n",
        "\n",
        "    # If we select the finished beams, we will have to add some padding.\n",
        "    padding = torch.tensor([tokenizer.pad_token_id], device=logits.device)\n",
        "\n",
        "    for i in range(self.num_beams):\n",
        "      # We will now select beam i for the next step.\n",
        "      # To do this, we compare the best finished beam from the previous step to\n",
        "      # the best of the expanded unfinished beams, and select the best of those two.\n",
        "      # (We also have to check whether we are out of finished beams.)\n",
        "      if next_finished_idx >= finished_log_probs.shape[0] \\\n",
        "         or expanded_sorted.values[next_unfinished_idx] > finished_log_probs[next_finished_idx]:\n",
        "        # We select the next best unfinished beam:\n",
        "\n",
        "        # First, we compute the index among the unfinished beams of the \n",
        "        # highest-scoring candidate.\n",
        "        seq_idx = torch.div(expanded_sorted.indices[next_unfinished_idx], logits.shape[-1], rounding_mode=\"floor\")\n",
        "\n",
        "        # Next, we compute the index in the vocabulary of the highest-scoring candidate.\n",
        "        next_token = expanded_sorted.indices[next_unfinished_idx] % logits.shape[-1]\n",
        "\n",
        "        # *YOUR WORK*: create a tensor next_beam where you add the next token id\n",
        "        # to the corresponding beam from the previous step.\n",
        "        # *HINT*: next_token is an integer while the previous beam is 1-dimensional.\n",
        "        # You may use the trick [None] as above to make next_token 1-dimensional.\n",
        "        next_beam = # WRITE CODE HERE\n",
        "        assert(next_beam.shape == torch.Size([n_tokens+1]))\n",
        "\n",
        "        next_unfinished_idx += 1\n",
        "      else:\n",
        "        # We select the next best previously finished beam:\n",
        "\n",
        "        # *YOUR WORK*: create a tensor next_beam where you add padding to the\n",
        "        # beam from the previous step.\n",
        "        next_beam = # WRITE CODE HERE\n",
        "        assert(next_beam.shape == torch.Size([n_tokens+1]))\n",
        "\n",
        "        next_finished_idx += 1\n",
        "\n",
        "      # Add the current beam to the list of selected beams.\n",
        "      beams.append(next_beam)      \n",
        "    \n",
        "    # *YOUR WORK*: Finally, concatenate all beams into a tensor and return it.\n",
        "    # The function torch.stack is probably going to be useful.\n",
        "    # https://pytorch.org/docs/stable/generated/torch.stack.html\n",
        "    next_input_ids = # WRITE CODE HERE\n",
        "\n",
        "    assert(next_input_ids.shape == torch.Size([self.num_beams, n_tokens+1]))    \n",
        "    return next_input_ids\n"
      ],
      "metadata": {
        "id": "Yt63OcUgC-C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell tests your code by carrying out one step of the beam search. The result should be a tensor of shape (5, 3). The generated texts will also be printed."
      ],
      "metadata": {
        "id": "2cYDNbZv-CS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This test should pass without modification.\n",
        "\n",
        "# We assume that the result from the previous step has the shape [3, 2].\n",
        "# The third of them ends with a period so we will consider this to be \"finished\".\n",
        "test_beams = tokenizer(['This is', 'That is', 'End.'], return_tensors='pt').input_ids.to(model.device)\n",
        "\n",
        "# Apply the model to compute the logits for the next tokens.\n",
        "test_logits = model(test_beams).logits\n",
        "\n",
        "# We will use a beam search with width 5 and a stopping criterion that finished after one sentence.\n",
        "beam_strategy = BeamSearchStrategy(num_beams=5, stopping_criterion=partial(has_n_sentences, n=1))\n",
        "\n",
        "# Apply one step of the beam search.\n",
        "new_beams = beam_strategy.step(test_logits, test_beams)\n",
        "\n",
        "# The result should have 5 rows (because num_beams is 5) and 3 columns (because we added one column).\n",
        "assert(new_beams.shape == torch.Size([5, 3]))\n",
        "\n",
        "# Finally, print the result:\n",
        "for beam in new_beams:\n",
        "  print(tokenizer.decode(beam))"
      ],
      "metadata": {
        "id": "1ZSwzRMp9_Lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let us finally use this to generate running text using beam search:"
      ],
      "metadata": {
        "id": "d0HgS97N-z6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopping_criterion = partial(has_n_sentences, n=2)\n",
        "beam_strategy = BeamSearchStrategy(num_beams=5, stopping_criterion=stopping_criterion)\n",
        "generated_ids = generate(\"NLP stands for natural\", beam_strategy, stopping_criterion=stopping_criterion)"
      ],
      "metadata": {
        "id": "W7jrrY8s5B7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We compute the joint probability again:"
      ],
      "metadata": {
        "id": "dMruS0ID-5n_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model(input_ids=generated_ids)\n",
        "beamsearch_joint_logprob = get_joint_log_probability(predictions.logits, generated_ids)\n",
        "\n",
        "print('Joint log probability of the text using beam search:', beamsearch_joint_logprob[0].item())"
      ],
      "metadata": {
        "id": "FGVtQFBGMYNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Investigating longer texts\n",
        "\n",
        "When we tested greedy and beam search decoding above, we used a stopping criterion that terminates the generation when two sentences have been produced.\n",
        "\n",
        "Let us see what happens when we generate longer text. Set the number of generated sentences to a larger value and generate again using beam search and greedy decoding and see if you can make any observation about the behavior.\n",
        "\n",
        "(This will be discussed in the individual reflection.)"
      ],
      "metadata": {
        "id": "GR9l7fx3O0-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WRITE CODE HERE"
      ],
      "metadata": {
        "id": "7DfI4R5NO2Ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random sampling\n",
        "\n",
        "Instead of searching for the most probable string, we could instead simply sample from the next token distribution.\n",
        "\n",
        "**Hint:** To sample from a given discrete distribution in PyTorch, you can build a [`Categorical`](https://pytorch.org/docs/stable/distributions.html#torch.distributions.categorical.Categorical) distribution and then use that to generate random numbers by calling the method `sample`.\n",
        "\n",
        "**Your work:** Implement the random sampling strategy below:"
      ],
      "metadata": {
        "id": "8kU-JTpveBgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributions import Categorical\n",
        "\n",
        "class RandomSamplingStrategy(DecodingStrategy):\n",
        "\n",
        "  def step(self, logits, input_ids):\n",
        "    # Let's define some auxiliary variables we will use in sanity checks.\n",
        "    batch_size, n_tokens, voc_size = logits.shape    \n",
        "\n",
        "    # *YOUR WORK*: Select the logits for the next token.\n",
        "    next_token_logits = # WRITE CODE HERE\n",
        "    assert(next_token_logits.shape == torch.Size([batch_size, voc_size]))    \n",
        "\n",
        "    # *YOUR WORK*: Select the next tokens randomly from the distribution\n",
        "    # defined by next_token_logits.\n",
        "    next_tokens = # WRITE CODE HERE\n",
        "    assert(next_tokens.shape == torch.Size([batch_size]))\n",
        "\n",
        "    # *YOUR WORK*: Add the new tokens to the previous input_ids.\n",
        "    next_input_ids = # WRITE CODE HERE\n",
        "    assert(next_input_ids.shape == torch.Size([batch_size, n_tokens+1]))\n",
        "    return next_input_ids"
      ],
      "metadata": {
        "id": "vLRs_25CMuAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's apply the random sampling strategy:"
      ],
      "metadata": {
        "id": "VLI0Q1JPDhB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_strategy = RandomSamplingStrategy()\n",
        "generated_ids = generate(\"NLP stands for natural\", random_strategy, stopping_criterion=partial(has_n_sentences, n=2))\n",
        "assert(generated_ids.shape[0] == 1)"
      ],
      "metadata": {
        "id": "3jIFg5hYgXUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top-_k_ sampling\n",
        "\n",
        "We can think of strategies that are a \"middle ground\" between maximum probability strategies, and random sampling. One such example is the **top-k** sampling strategy. In this strategy, we sample from the _top-k_ most probable next tokens. This means we normalize the probabilities of the k most probable next tokens, and sample from this new distribution.\n",
        "\n",
        "**Your work:** Implement the top-k sampling strategy below:"
      ],
      "metadata": {
        "id": "d3-3Yj9hhEhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TopKSamplingStrategy(DecodingStrategy):\n",
        "\n",
        "  def __init__(self, k: int):\n",
        "    self.k = k\n",
        "\n",
        "  def step(self, logits, input_ids):\n",
        "    # Let's define some auxiliary variables we will use in sanity checks.\n",
        "    batch_size, n_tokens, voc_size = logits.shape    \n",
        "\n",
        "    # *YOUR WORK*: Select the logits for the next token.\n",
        "    next_token_logits = # WRITE CODE HERE\n",
        "    assert(next_token_logits.shape == torch.Size([batch_size, voc_size]))\n",
        "\n",
        "    # *YOUR WORK*: Now, select the top k alternatives for every item in the batch.\n",
        "    # *Hint*: probably easiest to use the function topk here:\n",
        "    # https://pytorch.org/docs/stable/generated/torch.topk.html\n",
        "    topk = # WRITE CODE HERE\n",
        "    assert(topk.values.shape == torch.Size([batch_size, self.k]))\n",
        "\n",
        "    # *YOUR WORK*: Sample from among the top k candidates you found in the \n",
        "    # previous step.\n",
        "    index_in_topk = # WRITE CODE HERE\n",
        "    assert(index_in_topk.shape == torch.Size([batch_size]))\n",
        "\n",
        "    # By calling torch.gather, we can map the index in the top-k list back to \n",
        "    # the index of the vocabulary.\n",
        "    next_tokens = torch.gather(topk.indices, 1, index_in_topk[:, None])\n",
        "    assert(next_tokens.shape == torch.Size([batch_size, 1]))\n",
        "\n",
        "    # *YOUR WORK*: Concatenate the new generated tokens to the previous input_ids.\n",
        "    next_input_ids = # WRITE CODE HERE\n",
        "    assert(next_input_ids.shape == torch.Size([batch_size, n_tokens+1]))\n",
        "\n",
        "    return next_input_ids"
      ],
      "metadata": {
        "id": "uRJwYygSMuI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now use the top-$k$ sampling strategy to generate text. How do you think this compares to the previous decoding strategies?"
      ],
      "metadata": {
        "id": "ia6hWP0YQr8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_k_strategy = TopKSamplingStrategy(k=5)\n",
        "generated_ids = generate(\"NLP stands for natural\", top_k_strategy, stopping_criterion=partial(has_n_sentences, n=4))"
      ],
      "metadata": {
        "id": "qkE1VkoGleXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting it all together\n",
        "\n",
        "We have now implemented 4 different decoding strategies. Let's put them side by side to compare them more easily.\n"
      ],
      "metadata": {
        "id": "pZ39MkmEwFa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"NLP stands for natural\"\n",
        "\n",
        "stopping_criterion = partial(has_n_sentences, n=3)\n",
        "\n",
        "greedy_strategy = GreedyStrategy()\n",
        "beam_strategy = BeamSearchStrategy(num_beams=5, stopping_criterion=stopping_criterion)\n",
        "random_strategy = RandomSamplingStrategy()\n",
        "top_k_strategy = TopKSamplingStrategy(k=5)\n",
        "\n",
        "print(\"Greedy:\")\n",
        "print(\"-------\")\n",
        "generated_ids = generate(prompt, greedy_strategy, stopping_criterion=stopping_criterion, print_output=False)\n",
        "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n",
        "print()\n",
        "print()\n",
        "print(f\"Beam search ({beam_strategy.num_beams} beams):\")\n",
        "print(\"----------------------\")\n",
        "generated_ids = generate(prompt, beam_strategy, stopping_criterion=stopping_criterion, print_output=False)\n",
        "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True ))\n",
        "print()\n",
        "print()\n",
        "print(f\"Random sampling:\")\n",
        "print(\"----------------\")\n",
        "generated_ids = generate(prompt, random_strategy, stopping_criterion=stopping_criterion, print_output=False)\n",
        "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True ))\n",
        "print()\n",
        "print()\n",
        "print(f\"Top-k sampling (k={top_k_strategy.k}):\")\n",
        "print(\"---------------------\")\n",
        "generated_ids = generate(prompt, top_k_strategy, stopping_criterion=stopping_criterion, print_output=False)\n",
        "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True ))\n",
        "print()\n",
        "print()"
      ],
      "metadata": {
        "id": "Byk22gt7lvbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your work:**\n",
        "Play around with different settings for sequence length (e.g. how many sentences to generate), number of beams, and $k$ to get a feeling of how the algorithms behave. Also try modifying the prompt to something of your choosing."
      ],
      "metadata": {
        "id": "13-v2KVu0tiV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H8RxNi-9QfaD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}